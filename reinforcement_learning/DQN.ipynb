{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DQN Reinforced Learning\n",
        "\n",
        "Author: Rafael Godoy\n",
        "\n",
        "LinkedIn: https://www.linkedin.com/in/Rafael-Godoy-ML-Eng\n",
        "\n",
        "Github: https://github.com/RafaelSilvaGodoy\n",
        "\n",
        "\n",
        "This notebook will show a **Reinforced Learning** method called **DQN**. An agent is trained (green) by playing against Atari (orange) on a ping-pong game.\n",
        "\n",
        "The ping-pong game ends when the first player reaches 21 points.\n",
        "\n",
        "This notebook is prepared to run on Colab.\n",
        "\n",
        "The Atari ping-pong enviroment can be found on: https://github.com/openai/gym\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cCnUpWPJevXd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.linalg\n",
        "from PIL import Image\n",
        "from random import random, randrange, randint\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Lambda\n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "w640uE3wevZk"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = 'PongDeterministic-v4'      # Environment Name\n",
        "RENDER_VIDEO = False                   # Whether to render video during training\n",
        "IMG_SIZE = (105, 80)                   # Image Size after preprocessing (height, width)\n",
        "NET_INPUT_SHAPE = (105, 80, 3)         # Network Input Shape\n",
        "MAX_EPISODES = 201                     # Maximum Number of Episodes to be simulated\n",
        "MAX_TIMESTEPS_PER_EPISODE = np.inf     # Maximum number of timsteps in a single episode\n",
        "EXPERIENCE_REPLAY_SIZE = 1.5e5         # Maximum Size of Experience Replay Memory\n",
        "EXPERIENCE_START_SIZE = 50e3           # Minimum number of random transitions to store before training\n",
        "MINIBATCH_SIZE = 32                    # Size of minibatch sampled during training\n",
        "TARGET_NETWORK_UPDATE_FREQ = 1e3       # Rate at which DQN_target is synchronized with DQN\n",
        "LEARNING_RATE = 0.00025                # Learning rate used during training\n",
        "DISCOUNT_FACTOR = 0.99                 # MDP discount factor gamma\n",
        "DROPOUT_PROB = 0.1                     # Dropout rate for the DQN\n",
        "INITIAL_EPSILON = 1                    # Initial value of epsilon for epsilon-greedy policy\n",
        "EPSILON_DECREASE_RATE = 5e-6           # Epsilon decrease rate\n",
        "MIN_EPSILON = 0.1                      # Minimum epsilon value to guarantee minimum exploration later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoMBcHffHl56",
        "outputId": "f8e20612-95d7-424c-b937-a7d64b2f9623"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Roms.rar', <http.client.HTTPMessage at 0x7f6fadc1d6d0>)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hrlELRqxhgsb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "!mkdir rars\n",
        "!mv HC\\ ROMS.zip   rars\n",
        "!mv ROMS.zip  rars\n",
        "!python -m atari_py.import_roms rars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JLwyRY4Gevbl"
      },
      "outputs": [],
      "source": [
        "# Create Environment\n",
        "env = gym.make('PongDeterministic-v4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffXfTpQMhlRd",
        "outputId": "f5d4c8f9-a447-430d-bf33-b7c825e79af5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation Sapce:  (210, 160, 3)\n",
            "\n",
            "Action space:  Discrete(6)\n",
            "\n",
            "Action Meanings:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
          ]
        }
      ],
      "source": [
        "# Observation Space\n",
        "print(\"Observation Sapce: \", env.observation_space.shape)\n",
        "print(\"\")\n",
        "\n",
        "# Action Space\n",
        "print(\"Action space: \", env.action_space)\n",
        "print(\"\")\n",
        "\n",
        "# Action Meanings\n",
        "print('Action Meanings: ',env.unwrapped.get_action_meanings())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwTN1nxyjC-r"
      },
      "source": [
        "# Auxiliar Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "baLBHd4UiGEm"
      },
      "outputs": [],
      "source": [
        "# ImagePreprocessor\n",
        "\n",
        "class ImagePreprocessor(gym.Wrapper):\n",
        "    \n",
        "    def __init__(self, env, height, width, grayscale=True):\n",
        "        \n",
        "        super().__init__(env)\n",
        "        \n",
        "        self.height = int(height)\n",
        "        self.width = int(width)\n",
        "        self.grayscale = bool(grayscale)\n",
        "        \n",
        "        # check original shape / dtype\n",
        "        shape = self.env.observation_space.shape\n",
        "        dtype = self.env.observation_space.dtype\n",
        "        assert len(shape) == 3, \"bad shape: {}\".format(shape)\n",
        "        assert shape[2] == 3, \"bad shape: {}\".format(shape)\n",
        "        assert dtype == 'uint8', \"bad dtype: {}\".format(dtype)\n",
        "        \n",
        "        # update observation space\n",
        "        if self.grayscale:\n",
        "            shape = (self.height, self.width)\n",
        "        else:\n",
        "            shape = (self.height, self.width, shape[2])\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            shape=shape, low=0, high=255, dtype='uint8')\n",
        "\n",
        "\n",
        "    def _preprocess_frame(self, s):\n",
        "        img = Image.fromarray(s)\n",
        "        if self.grayscale:\n",
        "            img = img.convert('L')\n",
        "        img = img.resize((self.width, self.height))\n",
        "        return np.array(img)\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self._s_orig = self.env.reset()\n",
        "        s = self._preprocess_frame(self._s_orig)   # shape: [h, w]\n",
        "        return s\n",
        "\n",
        "\n",
        "    def step(self, a):\n",
        "        self._s_next_orig, r, done, info = self.env.step(a)\n",
        "        s_next = self._preprocess_frame(self._s_next_orig)\n",
        "        return s_next, r, done, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RrNNRmUkihuL"
      },
      "outputs": [],
      "source": [
        "# FrameStacker\n",
        "\n",
        "class FrameStacker(gym.Wrapper):\n",
        "    \n",
        "    def __init__(self, env, num_frames=4):\n",
        "        \n",
        "        super().__init__(env)\n",
        "\n",
        "        self.num_frames = int(num_frames)\n",
        "        \n",
        "        s = self.env.observation_space.sample()\n",
        "        if s.ndim == 2:\n",
        "            self._perm = (1, 2, 0)\n",
        "        elif s.ndim == 3:\n",
        "            self._perm = (1, 2, 3, 0)\n",
        "        else:\n",
        "            print(\"expected ndim equal to 2 or 3, got shape: {}\".format(s.shape))\n",
        "        \n",
        "        # update observation space\n",
        "        shape = s.shape + (self.num_frames,)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            shape=shape, low=0, high=255, dtype='uint8')\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        frame_shape = tuple(self.env.observation_space.shape)  # [h, w, c?]\n",
        "        shape = (self.num_frames,) + frame_shape               # [f, h, w, c?]\n",
        "        self._frames = np.zeros(shape, self.observation_space.dtype)\n",
        "        self._s_orig = self.env.reset()             # shape: [h, w, c?]\n",
        "        s = np.expand_dims(self._s_orig, axis=0)    # shape: [1, h, w, c?]\n",
        "        self._frames[...] = s                       # broadcast along axis=0\n",
        "        s = np.transpose(self._frames, self._perm)  # to shape: [h, w, c?, f]\n",
        "        return s\n",
        "\n",
        "\n",
        "    def step(self, a):\n",
        "        self._s_next_orig, r, done, info = self.env.step(a)\n",
        "        self._frames = np.roll(self._frames, -1, axis=0)\n",
        "        self._frames[-1] = self._s_next_orig\n",
        "        s_next = np.transpose(self._frames, self._perm)  # shape: [h, w, c?, f]\n",
        "        return s_next, r, done, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "R64wgMBnimsQ"
      },
      "outputs": [],
      "source": [
        "env = ImagePreprocessor(env, height=105, width=80, grayscale=True) # s shape: [h, w]\n",
        "env = FrameStacker(env, num_frames=3) # s shape: [h, w, f]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2TZ-kBeSisYH"
      },
      "outputs": [],
      "source": [
        "def generate_gif(frames_list, duration=50):\n",
        "    \"\"\"\n",
        "    Store a gif from the episode frames.\n",
        "    Parameters\n",
        "    ----------\n",
        "    frames_list: List of \"env.render(mode='rgb_array')\" frames to save.\n",
        "    duration : float, optional\n",
        "        Time between frames in the animated gif, in milliseconds.\n",
        "    \"\"\"\n",
        "    \n",
        "    print('Generating GIF')\n",
        "\n",
        "    filepath = 'gifs/game_b.gif'\n",
        "\n",
        "    # Process frames\n",
        "    for i in range(0,len(frames_list)):\n",
        "        preprocessed_frame = frames_list[i]\n",
        "        frame = Image.fromarray(preprocessed_frame)\n",
        "        frame = frame.convert('P', palette=Image.ADAPTIVE)\n",
        "        frames_list[i] = frame\n",
        "\n",
        "    # generate gif\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "    frames_list[0].save(\n",
        "                    fp=filepath, format='GIF', append_images=frames_list[1:], save_all=True,\n",
        "                    duration=duration, loop=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2kogKBDjAI1"
      },
      "source": [
        "# Random Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hYzmRs_Qixbj"
      },
      "outputs": [],
      "source": [
        "class RandomAgent(object):\n",
        "\n",
        "    def __init__(self, action_space):\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def act(self, observation, reward, done):\n",
        "        return self.action_space.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1ikZzGDSjBpC"
      },
      "outputs": [],
      "source": [
        "def simulate_random_agent(env, N_episodes):\n",
        "    '''\n",
        "    env: OpenAI gym environment.\n",
        "    N_episodes: Number of episodes to simulate random agent.\n",
        "    '''\n",
        "    print(\"Ona match consists in 21 scores. Agent point is +1, Atari is -1 point\")\n",
        "    agent = RandomAgent(env.action_space)\n",
        "    reward = 0\n",
        "    done = False\n",
        "    frames_list = []\n",
        "    agent_score = 0\n",
        "\n",
        "    for i in range(N_episodes):\n",
        "      ob = env.reset()\n",
        "      N_score = 0\n",
        "      while True:\n",
        "        action = agent.act(ob, reward, done)\n",
        "        ob, reward, done, _ = env.step(action)\n",
        "        N_score += reward\n",
        "        if reward>0:\n",
        "          agent_score += reward\n",
        "        if not i:\n",
        "          frames_list.append(env.render(mode='rgb_array'))\n",
        "          \n",
        "        if done:\n",
        "          print(\"Match points: \",N_score)\n",
        "          if not i:\n",
        "            generate_gif(frames_list)\n",
        "          break\n",
        "    \n",
        "\n",
        "    return agent_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random Agent\n",
        "The random agent choose randomly an action to be played."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDy55GEijfml",
        "outputId": "8a3a6e73-e3a0-4be5-9c11-2a32cc7b5b9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ona match consists in 21 scores. Agent point is +1, Atari is -1 point\n",
            "Match points:  -20.0\n",
            "Generating GIF\n",
            "Match points:  -20.0\n",
            "Match points:  -20.0\n",
            "Match points:  -21.0\n",
            "Match points:  -20.0\n",
            "Match points:  -19.0\n",
            "Match points:  -20.0\n",
            "Match points:  -21.0\n",
            "Match points:  -21.0\n",
            "Match points:  -21.0\n",
            "Total Score made by the agent in 10 episodes = 7.0\n"
          ]
        }
      ],
      "source": [
        "N_score = simulate_random_agent(env, N_episodes=10)\n",
        "print(\"Total Score made by the agent in 10 episodes = \" + str(N_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "Xg2mWVoQpBTV",
        "outputId": "4d61fd79-a60b-4ec4-9e10-9979f38555d6"
      },
      "outputs": [],
      "source": [
        "# First match Random agent (green) x Atari (orange)\n",
        "from IPython.display import Image as gif\n",
        "gif(open('/content/gifs/game_b.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikVv8XulpWBF"
      },
      "source": [
        "# DQN Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FTUM6X90pC-n"
      },
      "outputs": [],
      "source": [
        "def diff_transform_matrix(num_frames):\n",
        "    assert isinstance(num_frames, int) and num_frames >= 1\n",
        "    s = np.diag(np.power(-1, np.arange(num_frames)))  # alternating sign\n",
        "    m = s.dot(scipy.linalg.pascal(num_frames, kind='upper'))[::-1, ::-1]\n",
        "    M = K.constant(m, dtype='float32')\n",
        "    return M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zYtFr41Spmxc"
      },
      "outputs": [],
      "source": [
        "class DQNetwork:\n",
        "    def __init__(self,\n",
        "                 name,\n",
        "                 actions,\n",
        "                 input_shape,\n",
        "                 learning_rate=0.00025,\n",
        "                 discount_factor=0.99,\n",
        "                 minibatch_size=32,\n",
        "                 dropout_prob=0.1):\n",
        "        \n",
        "        # Parameters\n",
        "        self.name = name\n",
        "        self.actions = actions  # Size of the network output\n",
        "        self.discount_factor = discount_factor  # Discount factor of the MDP\n",
        "        self.minibatch_size = minibatch_size  # Size of the training batches\n",
        "        self.learning_rate = learning_rate  # Learning rate\n",
        "        self.dropout_prob = dropout_prob  # Probability of dropout\n",
        "        \n",
        "        def diff_transform(S):\n",
        "            S = K.cast(S, 'float32') / 255\n",
        "            M = diff_transform_matrix(num_frames=3)\n",
        "            return K.dot(S, M)\n",
        "        \n",
        "        \n",
        "        S = keras.Input(name='S', shape=input_shape, dtype='uint8')\n",
        "        # Lambda Layer\n",
        "        lambda_layer = Lambda(diff_transform)(S)\n",
        "        # Conv2D Layers\n",
        "        conv_layer_1 = Conv2D(filters=16, kernel_size=8, strides=4, activation='relu')(lambda_layer)\n",
        "        conv_layer_2 = Conv2D(filters=32, kernel_size=4, strides=2, activation='relu')(conv_layer_1)\n",
        "        # Flatten\n",
        "        flatten_layer = Flatten()(conv_layer_2)\n",
        "        # Dense Layer\n",
        "        X = Dense(units=256, activation='relu')(flatten_layer)\n",
        "        # Output Layer\n",
        "        Q = keras.layers.Dense(units=actions,\n",
        "                               activation='linear',\n",
        "                               kernel_initializer='zeros',\n",
        "                               name='Q')(X)\n",
        "        self.model = keras.Model(inputs=S, outputs=Q)\n",
        "        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
        "                           loss='huber_loss')\n",
        "    \n",
        "    \n",
        "    def train(self, batch, DQN_target):\n",
        "        \"\"\"\n",
        "        Generates inputs and targets from the given batch, trains the model on\n",
        "        them.\n",
        "        \n",
        "        batch: iterable of dictionaries with keys 'state', 'action',\n",
        "        'next_state', 'reward', 'final'\n",
        "        DQN_target: a target model to generate targets\n",
        "        \"\"\"\n",
        "        \n",
        "        States = [datapoint['state'] for datapoint in batch]\n",
        "        Next_States = [datapoint['next_state'] for datapoint in batch]\n",
        "        Q_train = []\n",
        "        \n",
        "        # Apply DQN and DQN_target to every datapoint simultaneously to reduce computation time\n",
        "        Current_States_Q_Values = self.model(tf.concat(States,axis=0)).numpy()   # Current States\n",
        "        Next_States_Q_Values = DQN_target(tf.concat(Next_States,axis=0)).numpy() # Next States\n",
        "        Next_States_Max_Q_Values = np.max(Next_States_Q_Values,axis=1)           # Max Q Values\n",
        "        \n",
        "        # Build Q_train targets\n",
        "        for i in range(0,len(batch)):\n",
        "            datapoint = batch[i]\n",
        "            # The error must be 0 on all actions except the one taken\n",
        "            y = list(Current_States_Q_Values[i,:])\n",
        "            if datapoint['final']:\n",
        "                y[datapoint['action']] = datapoint['reward']\n",
        "            else:\n",
        "                y[datapoint['action']] = datapoint['reward'] + \\\n",
        "                                         self.discount_factor * Next_States_Max_Q_Values[i]\n",
        "            Q_train.append(y)\n",
        "            \n",
        "\n",
        "        # Prepare inputs and targets\n",
        "        S_train = np.asarray(States).squeeze()\n",
        "        Q_train = np.asarray(Q_train).squeeze()\n",
        "        \n",
        "        \n",
        "        # Train the model for one epoch\n",
        "        self.model.train_on_batch(S_train,\n",
        "                                  Q_train)\n",
        "        \n",
        "    \n",
        "    \n",
        "    def predict(self, state):\n",
        "        \"\"\"\n",
        "        Feeds state to the model, returns predicted Q-values.\n",
        "        state: a numpy.array with same shape as the network's input\n",
        "        \n",
        "        :return: numpy.array with predicted Q-values\n",
        "        \"\"\"\n",
        "        return self.model.predict(state, batch_size=1)\n",
        "    \n",
        "    \n",
        "    def save(self):\n",
        "        \"\"\"\n",
        "        Saves the model weights to disk.\n",
        "        :param filename: file to which save the weights (must end with \".h5\")\n",
        "        :param append: suffix to append after \"model\" in the default filename\n",
        "            if no filename is given\n",
        "        \"\"\"\n",
        "        self.model.save(self.name)\n",
        "\n",
        "    def load(self):\n",
        "        self.model.load_weights(self.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrxDkMekqTe9"
      },
      "source": [
        "# DQAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FajtI3x0qAUr"
      },
      "outputs": [],
      "source": [
        "class DQAgent:\n",
        "    def __init__(self,\n",
        "                 actions,\n",
        "                 network_input_shape=(4, 110, 84),\n",
        "                 replay_memory_size=1024,\n",
        "                 minibatch_size=32,\n",
        "                 learning_rate=0.00025,\n",
        "                 discount_factor=0.99,\n",
        "                 dropout_prob=0.1,\n",
        "                 epsilon=1,\n",
        "                 epsilon_decrease_rate=9e-7,\n",
        "                 min_epsilon=0.1):\n",
        "        \n",
        "        # Parameters\n",
        "        self.actions = actions  # Size of the discrete action space\n",
        "        self.network_input_shape = network_input_shape  # Shape of the DQN input\n",
        "        self.replay_memory_size = replay_memory_size  # Size of replay memory\n",
        "        self.minibatch_size = minibatch_size  # Size of a DQN minibatch\n",
        "        self.learning_rate = learning_rate  # Learning rate for the DQN\n",
        "        self.discount_factor = discount_factor  # Discount factor of the MDP\n",
        "        self.dropout_prob = dropout_prob  # Dropout probability of the DQN\n",
        "        self.epsilon = epsilon  # Probability of taking a random action\n",
        "        self.epsilon_decrease_rate = epsilon_decrease_rate  # See update_epsilon\n",
        "        self.min_epsilon = min_epsilon  # Minimum value for epsilon\n",
        "        \n",
        "        # Replay memory\n",
        "        self.experience_replay = []\n",
        "        self.training_count = 0\n",
        "        \n",
        "        # Instantiate the deep Q-networks\n",
        "        # Main DQN\n",
        "        self.DQN = DQNetwork(\n",
        "            'DQN',\n",
        "            self.actions,\n",
        "            self.network_input_shape,\n",
        "            learning_rate=self.learning_rate,\n",
        "            discount_factor=self.discount_factor,\n",
        "            minibatch_size=self.minibatch_size,\n",
        "            dropout_prob=self.dropout_prob)\n",
        "        \n",
        "        # Target DQN used to generate targets\n",
        "        self.DQN_target = keras.models.clone_model(self.DQN.model)\n",
        "        self.DQN_target.trainable = False  # exclude from trainable weights\n",
        "        \n",
        "    \n",
        "    \n",
        "    def get_action(self, state, training=True, random_action=False):\n",
        "        \"\"\"\n",
        "        Polls DQN for Q-values. Returns argmax(Q) with probability 1-epsilon\n",
        "        during training, 0 during testing.\n",
        "        \n",
        "        state: a state that can be passed as input to DQN\n",
        "        training: whether to use the current epsilon or return greedy action\n",
        "        random_action: whether to sample a random action regardless of parameters\n",
        "        \n",
        "        :return: the index of (action associated to) the highest Q-value \n",
        "        \"\"\"\n",
        "\n",
        "        is_random = (random() < self.epsilon) if training else False\n",
        "        if random_action or is_random:\n",
        "            return randint(0, self.actions - 1)\n",
        "        else:\n",
        "            q_values = self.DQN.predict(state)\n",
        "            return np.argmax(q_values)\n",
        "        \n",
        "    \n",
        "    \n",
        "    def get_max_q(self, state):\n",
        "        \"\"\"\n",
        "        Returns the action corresponding to maximum Q value predicted on the given state.\n",
        "        :param state: a state that can be passed as input to DQN\n",
        "        :return: an action index corresponding to the maximum Q-value in the \n",
        "            given state\n",
        "        \"\"\"\n",
        "        q_values = self.DQN.predict(state)\n",
        "        idxs = np.argwhere(q_values == np.max(q_values)).ravel()\n",
        "        return np.random.choice(idxs)\n",
        "    \n",
        "    \n",
        "    def get_random_state(self):\n",
        "        \"\"\"\n",
        "        Samples a random state from the replay memory.\n",
        "        :return: the sampled state\n",
        "        \"\"\"\n",
        "        return self.experience_replay[randrange(0, len(self.experience_replay))]['state']\n",
        "    \n",
        "    \n",
        "    def add_experience(self, state, action, reward, next_state, final):\n",
        "        \"\"\"\n",
        "        Add a SARS' tuple to the experience replay.\n",
        "        :param state: current state\n",
        "        :param action: action index\n",
        "        :param reward: reward associated to the transition\n",
        "        :param next_state: next state\n",
        "        :param final: whether the state is absorbing\n",
        "        \"\"\"\n",
        "\n",
        "        if len(self.experience_replay) >= self.replay_memory_size:\n",
        "          self.experience_replay.pop(0)\n",
        "\n",
        "        self.experience_replay.append({'state': state,\n",
        "                                       'action': action,\n",
        "                                       'reward': reward,\n",
        "                                       'next_state': next_state,\n",
        "                                       'final': final})\n",
        "    \n",
        "    \n",
        "    def sample_batch(self):\n",
        "        \"\"\"\n",
        "        Samples self.minibatch_size random transitions from the replay memory\n",
        "        and returns them as a batch.\n",
        "        :return: a batch of SARS' tuples\n",
        "        \"\"\"\n",
        "        batch = []\n",
        "        \n",
        "        for i in range(self.minibatch_size):\n",
        "            batch.append(self.experience_replay[randrange(0, len(self.experience_replay))])\n",
        "\n",
        "        return np.asarray(batch)\n",
        "    \n",
        "    \n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Trains the DQN on a minibatch of transitions.\n",
        "        \"\"\"\n",
        "        self.training_count += 1\n",
        "#        print ('Training session #%d - epsilon: %f' % \\\n",
        "#              (self.training_count, self.epsilon))\n",
        "        batch = self.sample_batch()\n",
        "        \n",
        "        self.DQN.train(batch, self.DQN_target)  # Train the DQN\n",
        "    \n",
        "    \n",
        "    def update_epsilon(self):\n",
        "        \"\"\"\n",
        "        Decreases the probability of picking a random action, to improve\n",
        "        exploitation.\n",
        "        \"\"\"\n",
        "        if self.epsilon - self.epsilon_decrease_rate > self.min_epsilon:\n",
        "            self.epsilon -= self.epsilon_decrease_rate\n",
        "        else:\n",
        "            self.epsilon = self.min_epsilon\n",
        "    \n",
        "    \n",
        "    def sync_target_network(self):\n",
        "        \"\"\"\n",
        "        Updates the target DQN with the current weights of the main DQN.\n",
        "        \"\"\"\n",
        "        print('Synchronizing Target Network')\n",
        "        self.DQN_target.set_weights(self.DQN.model.get_weights())\n",
        "    \n",
        "    \n",
        "    def save_networks(self):\n",
        "        \"\"\"\n",
        "        Saves DQN and DQN_target.\n",
        "        \"\"\"\n",
        "        self.DQN.save()\n",
        "        self.DQN_target.save()\n",
        "\n",
        "    def load_weights(self):\n",
        "        self.DQN.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "HXCrOHEZqOr_"
      },
      "outputs": [],
      "source": [
        "# Create Agent\n",
        "DQA = DQAgent(env.action_space.n,\n",
        "              network_input_shape=NET_INPUT_SHAPE,\n",
        "              replay_memory_size=EXPERIENCE_REPLAY_SIZE,\n",
        "              minibatch_size=MINIBATCH_SIZE,\n",
        "              learning_rate=LEARNING_RATE,\n",
        "              discount_factor=DISCOUNT_FACTOR,\n",
        "              dropout_prob=DROPOUT_PROB,\n",
        "              epsilon=INITIAL_EPSILON,\n",
        "              epsilon_decrease_rate=EPSILON_DECREASE_RATE,\n",
        "              min_epsilon=MIN_EPSILON)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIAEQeFPqcq8"
      },
      "source": [
        "# GIF Auxiliar function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "t3BKW5f2qYqy"
      },
      "outputs": [],
      "source": [
        "def generate_gif_from_agent(env, agent, episode_index, resize_to=None, duration=50):\n",
        "    \"\"\"\n",
        "    Store a gif from the episode frames.\n",
        "    Parameters\n",
        "    ----------\n",
        "    env : gym environment\n",
        "        The environment to record from.\n",
        "    agent : DQAgent object\n",
        "        The agent with policy that is used to take actions.\n",
        "    episode_index : int\n",
        "        Episode Index (used only to name the gif)\n",
        "    resize_to : tuple of ints, optional\n",
        "        The size of the output frames, ``(width, height)``. Notice the\n",
        "        ordering: first **width**, then **height**. This is the convention PIL\n",
        "        uses.\n",
        "    duration : float, optional\n",
        "        Time between frames in the animated gif, in milliseconds.\n",
        "    \"\"\"\n",
        "    \n",
        "    print('Generating GIF')\n",
        "\n",
        "    filepath = 'gifs/ep{:06d}.gif'.format(episode_index)\n",
        "\n",
        "    # collect frames\n",
        "    frames = []\n",
        "    s = env.reset()\n",
        "    for t in range(env.spec.max_episode_steps or 10000):\n",
        "        a = agent.get_action(np.asarray([s]), training=False)\n",
        "        s_next, r, done, info = env.step(a)\n",
        "\n",
        "        # store frame\n",
        "        frame = env.render(mode='rgb_array')\n",
        "        frame = Image.fromarray(frame)\n",
        "        frame = frame.convert('P', palette=Image.ADAPTIVE)\n",
        "        if resize_to is not None:\n",
        "            if not (isinstance(resize_to, tuple) and len(resize_to) == 2):\n",
        "                raise TypeError(\"expected a tuple of size 2, resize_to=(w, h)\")\n",
        "            frame = frame.resize(resize_to)\n",
        "\n",
        "        frames.append(frame)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        s = s_next\n",
        "\n",
        "    # store last frame\n",
        "    frame = env.render(mode='rgb_array')\n",
        "    frame = Image.fromarray(frame)\n",
        "    frame = frame.convert('P', palette=Image.ADAPTIVE)\n",
        "    if resize_to is not None:\n",
        "        frame = frame.resize(resize_to)\n",
        "    frames.append(frame)\n",
        "\n",
        "    # generate gif\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "    frames[0].save(\n",
        "        fp=filepath, format='GIF', append_images=frames[1:], save_all=True,\n",
        "        duration=duration, loop=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "sVWiz5YwqfFE"
      },
      "outputs": [],
      "source": [
        "def plot_scores(scores_list):\n",
        "    plt.style.use('fivethirtyeight')\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.gca()\n",
        "    ax.plot(scores_list)\n",
        "    fig.suptitle('Training Scores', fontsize=16)\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Score = $ \\sum R_t $')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMXpqBhoqnNV"
      },
      "source": [
        "# Agent Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KZ_YAwII3xW",
        "outputId": "8fe642f2-7332-477f-aa38-ab5139630bf5"
      },
      "outputs": [],
      "source": [
        "# Set counters\n",
        "episode = 1\n",
        "frame_counter = 0\n",
        "scores_list = []\n",
        "\n",
        "# Main loop\n",
        "while episode < MAX_EPISODES:\n",
        "    # Start episode\n",
        "    print('-------------------------------------------------')\n",
        "    print('Episode: ' + str(episode))\n",
        "    score = 0\n",
        "    \n",
        "    # Initialize first state\n",
        "    current_state = env.reset()\n",
        "    \n",
        "    # Main episode loop\n",
        "    t = 0\n",
        "    frame_counter += 1\n",
        "    while t < MAX_TIMESTEPS_PER_EPISODE:\n",
        "        # Render the game\n",
        "        if RENDER_VIDEO:\n",
        "            env.render()\n",
        "        \n",
        "        # Select an action using the DQA\n",
        "        action = DQA.get_action(np.asarray([current_state]))\n",
        "\n",
        "        # Observe reward and next state\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        frame_counter += 1\n",
        "\n",
        "        # Store transition in replay memory\n",
        "        #clipped_reward = np.clip(reward, -1, 1)  # Clip the reward\n",
        "        DQA.add_experience(np.asarray([current_state]),\n",
        "                           action,\n",
        "                           reward,\n",
        "                           np.asarray([next_state]),\n",
        "                           done)\n",
        "        \n",
        "        # Train the agent\n",
        "        if len(DQA.experience_replay) >= EXPERIENCE_START_SIZE:\n",
        "            DQA.train()\n",
        "            # Every C DQN updates, update DQN_target\n",
        "            if DQA.training_count % TARGET_NETWORK_UPDATE_FREQ == 0 and DQA.training_count >= TARGET_NETWORK_UPDATE_FREQ:\n",
        "                DQA.sync_target_network()\n",
        "        \n",
        "        # Linear epsilon annealing\n",
        "        if len(DQA.experience_replay) >= EXPERIENCE_START_SIZE:\n",
        "            DQA.update_epsilon()\n",
        "\n",
        "        # Update the current state and score\n",
        "        current_state = next_state\n",
        "        score += reward\n",
        "\n",
        "        # Check end of episode\n",
        "        if done:\n",
        "            scores_list.append(score)\n",
        "            print(\"Length: %d; Score: %d\" % (t + 1, score))\n",
        "            print(\"Epsilon = \" + str(DQA.epsilon))\n",
        "            print(\"Total Frame Counter = \" + str(frame_counter))\n",
        "            break\n",
        "        \n",
        "        # Increment Episode Timestep\n",
        "        t += 1\n",
        "    \n",
        "    \n",
        "    # Save GIF with greedy policy\n",
        "    if (score > 3):\n",
        "        generate_gif_from_agent(env=env,\n",
        "                                agent=DQA,\n",
        "                                episode_index=episode,\n",
        "                                resize_to=(320, 420))\n",
        "    # Increment Episode counter\n",
        "    episode += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-hxt5S5qzwT"
      },
      "outputs": [],
      "source": [
        "plot_scores(scores_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2dYhQfvv5JT"
      },
      "outputs": [],
      "source": [
        "gif(open('/content/gifs/ep000200.gif','rb').read())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "iIAEQeFPqcq8"
      ],
      "name": "DQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
